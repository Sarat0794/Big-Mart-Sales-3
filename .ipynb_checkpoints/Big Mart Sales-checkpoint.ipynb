{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Importing the dataset\n",
    "dataset_train=pd.read_csv(\"Train_UWu5bXk.csv\")\n",
    "dataset_test=pd.read_csv(\"Test_u94Q5KV.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two different dataset are given for test and train. Therefore concatinating them for data preprocessing\n",
    "dataset=pd.concat([dataset_train,dataset_test], axis=0,ignore_index=True) #axis=0 adds the other dataset to the bottem of other dataset\n",
    "\n",
    "#Separating the dependent variable \"y\" from the dataset\n",
    "x=dataset.iloc[:,dataset.columns!='Item_Outlet_Sales'] #iloc: integer location\n",
    "y=dataset.loc[:,['Item_Outlet_Sales']] \n",
    "#import statistics\n",
    "#statistics.mean(dataset['Item_Visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()\n",
    "#Item Identifier,Outlet_Identifier might be useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Exploration\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the 0 visibity with the mean visibility\n",
    "import statistics\n",
    "statistics.mean(dataset['Item_Visibility'])\n",
    "dataset['Item_Visibility']=dataset['Item_Visibility'].replace(0,0.06595278007399324)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Key Observations:\n",
    "    # 1) minimum Item Visibility is 0, i.e that particular item may not be present in that store. 0 can also be a MISSING VALUE. For now replacing the 0 with mean in the above code.\n",
    "    # 2) There are 4 continuous variables and 8 categorical values\n",
    "    # 3) The outlet_sales are not given for the test dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FIRST WE WILL DEAL WITH THE CATEGORICAL VARIABLES\n",
    "TO DO: Null value check, Wrongly spelled categories\n",
    "1) Item_Fat_Content\n",
    "2) Item_Type\n",
    "3) Outlet_Location_Type\n",
    "4) Outlet_Size\n",
    "5) Outlet_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring each categorical variable\n",
    "dataset['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dataset['Item_Fat_Content'].isnull())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Observations:\n",
    "    # 1) Low Fat is also represented as LF and low fat\n",
    "    # 2) Regular is also represented as reg \n",
    "    # 3) No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing LF and low fat with Low Fat and reg with Regular\n",
    "dataset['Item_Fat_Content']=dataset['Item_Fat_Content'].replace(['LF','low fat','reg'],['Low Fat','Low Fat','Regular'])\n",
    "dataset['Item_Fat_Content'].value_counts()\n",
    "##########################################################################################################################\n",
    "# 1st categorical variable is solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of missing values= %s'%(sum(dataset['Item_Type'].isnull())))\n",
    "print('/n')\n",
    "print(dataset['Item_Type'].value_counts())\n",
    "##########################################################################################################################\n",
    "#Nothing yet to solve for 2nd categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of missing values= %s'%(sum(dataset['Outlet_Location_Type'].isnull())))\n",
    "print('/n')\n",
    "dataset['Outlet_Location_Type'].value_counts() \n",
    "##########################################################################################################################\n",
    "#Nothing yet to solve for 3rd categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of missing values= %s'%(sum(dataset['Outlet_Size'].isnull())))\n",
    "print('/n')\n",
    "dataset['Outlet_Size'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are 4016 missing values in  the Outlet_Size column\n",
    "    STEPS TO IMPUTE THE MISSING VALUES OF THIS COLUMN: \n",
    "        1) Put all the null values in Outlet_Size column into one variable.\n",
    "        2) Put all the non-null values in Outlet_Size into other variable. \n",
    "        3) Check if ther are any missing values in other features. \n",
    "        4) Using proper strategies to impute those missing values \n",
    "        5) Then apply ML algorithm to impute missing values in Outlet_Size column.\n",
    "        \n",
    "############################################### Do It Later ################################################################ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-1\n",
    "#Putting all null values separately\n",
    "#Outlet_Size_Nulls=dataset.loc[dataset['Outlet_Size'].isnull()]\n",
    "#STEP-2\n",
    "#Putting all non-null values separately\n",
    "#Outlet_Size_NotNulls=dataset.loc[dataset['Outlet_Size'].notnull()]\n",
    "#STEP-3\n",
    "#Outlet_Size_NotNulls.isnull().any()\n",
    "#Item_Weight may not affect the Outlet_Size. Therefore dropping that column. \n",
    "#Outlet_Sales missing values are from test set. Therefore, removing that column too assuming that it have no affect on the Outlet_Size\n",
    "#STEP-4\n",
    "#Dropping the variables with missing values, which are assumed to have no effect on the Outlet_Size.\n",
    "#Outlet_Size_NotNulls=Outlet_Size_NotNulls.drop(['Item_Weight'], axis=1)\n",
    "#Outlet_Size_NotNulls=Outlet_Size_NotNulls.drop(['Item_Outlet_Sales'], axis=1)\n",
    "# STEP-5 Creating matrix of variables\n",
    "#Outlet_Size_NotNulls_x=Outlet_Size_NotNulls.loc[:,['Outlet_Location_Type','Outlet_Type']]\n",
    "#Outlet_Size_NotNulls_y=Outlet_Size_NotNulls.loc[:,'Outlet_Size']\n",
    "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "#labelencoder_x=LabelEncoder()\n",
    "#Outlet_Size_NotNulls_x['Outlet_Location_Type']=labelencoder_x.fit_transform(Outlet_Size_NotNulls_x['Outlet_Location_Type'])\n",
    "#Outlet_Size_NotNulls_x['Outlet_Type']=labelencoder_x.fit_transform(Outlet_Size_NotNulls_x['Outlet_Type'])\n",
    "#onehotencoder=OneHotEncoder()\n",
    "#Outlet_Size_NotNulls_x=onehotencoder.fit_transform(Outlet_Size_NotNulls_x).toarray()\n",
    "#labelencoder_y=LabelEncoder()\n",
    "#Outlet_Size_NotNulls_y=labelencoder_y.fit_transform(Outlet_Size_NotNulls_y)\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#Outlet_Size_NotNulls_x_train,Outlet_Size_NotNulls_x_test,Outlet_Size_NotNulls_y_train,Outlet_Size_NotNulls_y_test=train_test_split(Outlet_Size_NotNulls_x,Outlet_Size_NotNulls_y,test_size=0.2,random_state=0)\n",
    "# TRying KNN\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#KNNclassifier=KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "#KNNclassifier.fit(Outlet_Size_NotNulls_x_train,Outlet_Size_NotNulls_y_train)\n",
    "#KNN_y_pred=KNNclassifier.predict(Outlet_Size_NotNulls_x_test)\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#Kcm=confusion_matrix(Outlet_Size_NotNulls_y_test, KNN_y_pred)\n",
    "#Trying SVM\n",
    "#from sklearn.svm import SVC\n",
    "#SVMclassifier=SVC(kernel='rbf', random_state=0)\n",
    "#SVMclassifier.fit(Outlet_Size_NotNulls_x_train,Outlet_Size_NotNulls_y_train)\n",
    "#SVM_y_pred=SVMclassifier.predict(Outlet_Size_NotNulls_x_test)\n",
    "#SVM_cm=confusion_matrix(Outlet_Size_NotNulls_y_test, SVM_y_pred)\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#classifier=LogisticRegression(random_state=0)\n",
    "#classifier.fit(Outlet_Size_NotNulls_x_train,Outlet_Size_NotNulls_y_train)\n",
    "#y_pred=classifier.predict(Outlet_Size_NotNulls_x_test)\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#cm=confusion_matrix(Outlet_Size_NotNulls_y_test, y_pred)\n",
    "#labelencoder_y.inverse_transform(y_pred)\n",
    "####################################################################################################\n",
    "#dataset1=dataset.loc[dataset['Outlet_Size'].notnull()]\n",
    "#all_cols= dataset.loc[:,['Outlet_Location_Type','Outlet_Type','Outlet_Size']]\n",
    "\n",
    "#x_trn = dataset1.loc[:,['Outlet_Location_Type','Outlet_Type']]\n",
    "#y_trn=dataset1['Outlet_Size']\n",
    "\n",
    "#dataset2=dataset.loc[dataset['Outlet_Size'].isnull()]\n",
    "#x_tst = dataset2.loc[:,['Outlet_Location_Type','Outlet_Type']]\n",
    "#y_tst=dataset2['Outlet_Size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####---------WORKED------------######\n",
    "\n",
    "# Filling the missing values in 'Outlet_Size' with the predicted values \n",
    "#Step-1: select only those variables that are assumed to have affect on 'Outlet_Size' prediction.\n",
    "#Step-2: do OHE for the selected variables \n",
    "dummy =pd.concat([pd.get_dummies(dataset[['Outlet_Location_Type','Outlet_Type','Outlet_Identifier']]), dataset[['Item_Visibility']]], axis=1)\n",
    "# Concatinating the 'Outlet_Size' to the selected variables dataset (dummy)\n",
    "dummy=pd.concat([dummy,dataset['Outlet_Size']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the datatypes of each variable after OHE\n",
    "dummy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-3: separating the rows with 'NaN' values into two defferent datasets (NO_NAN and NAN) \n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "# Dataset without NaN values.\n",
    "NO_NAN = dummy.loc[dummy['Outlet_Size'].notnull()]\n",
    "Input=NO_NAN.iloc[:,:-1]\n",
    "Output=NO_NAN.iloc[:,-1]\n",
    "#Output=pd.get_dummies(Output)\n",
    "\n",
    "#labelencoder_x=LabelEncoder()\n",
    "#Output=labelencoder_x.fit_transform(Output)\n",
    "\n",
    "# Dataset with NaN values.\n",
    "NAN =  dummy.loc[dummy['Outlet_Size'].isnull()]\n",
    "ip = NAN.iloc[:,:-1]\n",
    "op = NAN.iloc[:,-1]\n",
    "#labelencoder_y=LabelEncoder()\n",
    "#op=labelencoder_y.fit_transform(op)\n",
    "\n",
    "# Step-4: seperating the NO_NAN dataset into train and test for testing if the predictions are correct or not\n",
    "from sklearn.model_selection import train_test_split\n",
    "Input_train,Input_test,Output_train,Output_test=train_test_split(Input,Output,test_size=0.2,random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNNclassifier=KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "KNNclassifier.fit(Input_train,Output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values of NO_NAN test set\n",
    "KNN_y_pred=KNNclassifier.predict(Input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values in prediction of test set\n",
    "np.unique(KNN_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values for the predictions of null values\n",
    "np.unique(KNNclassifier.predict(ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix to check the predictions for NO_NAN test set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Kcm=confusion_matrix(Output_test, KNN_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kcm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Problems with ONE HOT ENCODER:\n",
    "    1) Could not select the required columns to convert. It is always considering the output variable too for conversion.\n",
    "    2) Do we have to do Label Encoding before OHE? What is the reason?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Train\n",
    "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "#labelencoder_1=LabelEncoder()\n",
    "#labelencoder_2=LabelEncoder()\n",
    "#dataset['Outlet_Location_Type']=labelencoder_1.fit_transform(dataset['Outlet_Location_Type'])\n",
    "#dataset['Outlet_Type']=labelencoder_2.fit_transform(dataset['Outlet_Type'])\n",
    "#onehotencoder_1=OneHotEncoder(categorical_features='all')\n",
    "#dataset=onehotencoder_1.fit_transform(dataset).toarray()\n",
    "#For Train\n",
    "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "#labelencoder_1=LabelEncoder()\n",
    "#labelencoder_2=LabelEncoder()\n",
    "#x_trn['Outlet_Location_Type']=labelencoder_1.fit_transform(x_trn['Outlet_Location_Type'])\n",
    "#x_trn['Outlet_Type']=labelencoder_2.fit_transform(x_trn['Outlet_Type'])\n",
    "#onehotencoder_1=OneHotEncoder(categorical_features='all')\n",
    "#x_trn=onehotencoder_1.fit_transform(x_trn).toarray()\n",
    "#labelencoder_3=LabelEncoder()\n",
    "#y_trn=labelencoder_3.fit_transform(y_trn)\n",
    "#x_trn=x_trn[:,1:] # Removing 1st column fot dummy variable trap\n",
    "#For Test (NUll Values)\n",
    "#labelencoder_4=LabelEncoder()\n",
    "#labelencoder_5=LabelEncoder()\n",
    "#x_tst['Outlet_Location_Type']=labelencoder_4.fit_transform(x_tst['Outlet_Location_Type'])\n",
    "#x_tst['Outlet_Type']=labelencoder_5.fit_transform(x_tst['Outlet_Type'])\n",
    "\n",
    "#onehotencoder_2=OneHotEncoder(categorical_features='all')\n",
    "#x_tst=onehotencoder_2.fit_transform(x_tst).toarray()\n",
    "\n",
    "#x_tst=x_tst[:,1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Outlet_Size'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It did not work: Have to check the problem later.\n",
    "#dataset['Outlet_Size'].fillna(0, inplace=True)\n",
    "#for i in range(0,len(dataset)):\n",
    "    #if dataset['Outlet_Size'][i]==0:\n",
    "        \n",
    "        #dataset['Outlet_Size'][i]=dataset['Outlet_Size'][i].replace(0, KNNclassifier.predict(dataset['Outlet_Size'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting all the predicted values for NaNs in a separate variable 'filler'\n",
    "filler=KNNclassifier.predict(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the array to a list \n",
    "filler=filler.tolist() # Converting an array into list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make sure that the number of predicted values for nulls and # of nulls are equal.\n",
    "sum(dataset['Outlet_Size'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIlling all the NaN values with the predicted values.\n",
    "dataset.loc[dataset.Outlet_Size.isnull(), 'Outlet_Size'] = filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the missing values with mode. \n",
    "#import statistics\n",
    "#dataset['Outlet_Size']=dataset['Outlet_Size'].fillna(statistics.mode(dataset['Outlet_Size']))\n",
    "##################### 4th categorical variable si kind of solved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Outlet_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of missing values in each feature\n",
    "sum(dataset['Outlet_Type'].isnull())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOW LET US DEAL WITH THE NUMERICAL VALUES\n",
    "TO DO: Null Value check, outliers.\n",
    "    1) Item_MRP\n",
    "    2) Item_Outlet_Sales\n",
    "    3) Item_Visibility\n",
    "    4) Item_Weight\n",
    "    5) Outlet_Establishment_Year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dataset['Item_MRP'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier check\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(dataset['Item_MRP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(dataset['Item_Visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
